rule diginorm_load_into_counting:
    """
    Build the hash table data structure from all the trimmed reads.
    Caution!: The --help says that it can be multithreaded but it raises
    errors!
    """
    input:
        fastqs = expand(
            QC_DIR + "{sample}.final.{pair}.fq.gz",
            sample = SAMPLES_PE,
            pair = PAIRS
        ) + expand(
            QC_DIR + "{sample}.final.se.fq.gz",
            sample = SAMPLES_SE
        )
    output:
        table = temp(NORM_DIR + "diginorm_table.kh"),
        info  = temp(NORM_DIR + "diginorm_table.kh.info")
    threads:
        ALL_THREADS
    priority:
        50
    log:
        NORM_DIR + "load_into_counting.log"
    benchmark:
        NORM_DIR + "load_into_counting.json"
    params:
        ksize= config["diginorm_params"]["ksize"],
        max_table_size= config["diginorm_params"]["max_table_size"],
        n_tables= config["diginorm_params"]["n_tables"]
    shell:
        "load-into-counting.py "
            "--ksize {params.ksize} "
            "--n_tables {params.n_tables} "
            "--max-tablesize {params.max_table_size} "
            "--no-bigcount "
            "--threads {threads} "
            "{output.table} "
            "{input.fastqs} "
        "> {log} 2>&1"



rule diginorm_normalize_by_median_sample_pe_pe:
    """
    Normalizes by median EACH FILE.
    Therefore one loads once per file the hash table.
    """
    input:
        fastq = QC_DIR + "{sample}.final.pe_pe.fq.gz",
        table = NORM_DIR + "diginorm_table.kh"
    output:
        fastq = temp(NORM_DIR + "{sample}.keep.pe_pe.fq.gz")
    threads:
        ALL_THREADS
    priority:
        50
    params:
        cutoff   = config["diginorm_params"]["cutoff"],
        ksize    = config["diginorm_params"]["ksize"],
        n_tables = config["diginorm_params"]["n_tables"],
        max_table_size = config["diginorm_params"]["max_table_size"],
    log:
        NORM_DIR + "normalize_by_median_{sample}.pe_pe.log"
    benchmark:
        NORM_DIR + "normalize_by_median_{sample}.pe_pe.json"
    shell:
        "normalize-by-median.py "
            "--ksize {params.ksize} "
            "--n_tables {params.n_tables} "
            "--max-tablesize {params.max_table_size} "
            "--cutoff {params.cutoff} "
            "--paired "
            "--loadgraph {input.table} "
            "--output >(pigz --best > {output.fastq}) "
            "<(pigz --decompress --stdout {input.fastq}) "
        "> {log} 2>&1 "



rule diginorm_normalize_by_median_sample_pe_se:
    """
    Normalizes by median EACH FILE.
    Therefore one loads once per file the hash table.
    """
    input:
        fastq = QC_DIR + "{sample}.final.pe_se.fq.gz",
        table = NORM_DIR + "diginorm_table.kh"
    output:
        fastq = temp(NORM_DIR + "{sample}.keep.pe_se.fq.gz")
    threads:
        ALL_THREADS
    priority:
        50
    params:
        cutoff   = config["diginorm_params"]["cutoff"],
        ksize    = config["diginorm_params"]["ksize"],
        n_tables = config["diginorm_params"]["n_tables"],
        max_table_size = config["diginorm_params"]["max_table_size"]
    log:
        NORM_DIR + "normalize_by_median_{sample}_pe_se.log"
    benchmark:
        NORM_DIR + "normalize_by_median_{sample}_pe_se.json"
    shell:
        "normalize-by-median.py "
            "--ksize {params.ksize} "
            "--n_tables {params.n_tables} "
            "--max-tablesize {params.max_table_size} "
            "--cutoff {params.cutoff} "
            "--loadgraph {input.table} "
            "--output >(pigz --best > {output.fastq}) "
            "<(pigz --decompress --stdout {input.fastq}) "
        "> {log} 2>&1"



rule diginorm_normalize_by_median_sample_se:
    """
    Normalizes by median EACH FILE.
    Therefore one loads once per file the hash table.
    """
    input:
        fastq = QC_DIR + "{sample}.final.se.fq.gz",
        table = NORM_DIR + "diginorm_table.kh"
    output:
        fastq = temp(NORM_DIR + "{sample}.keep.se.fq.gz")
    threads:
        ALL_THREADS
    priority:
        50
    params:
        cutoff   = config["diginorm_params"]["cutoff"],
        ksize    = config["diginorm_params"]["ksize"],
        n_tables = config["diginorm_params"]["n_tables"],
        max_table_size = config["diginorm_params"]["max_table_size"]
    log:
        NORM_DIR + "normalize_by_median_{sample}_se.log"
    benchmark:
        NORM_DIR + "normalize_by_median_{sample}_se.json"
    shell:
        "normalize-by-median.py "
            "--ksize {params.ksize} "
            "--n_tables {params.n_tables} "
            "--max-tablesize {params.max_table_size} "
            "--cutoff {params.cutoff} "
            "--loadgraph {input.table} "
            "--output >(pigz --best > {output.fastq}) "
            "<(pigz --decompress --stdout {input.fastq}) "
        "> {log} 2>&1"




rule diginorm_filter_abund_sample_pair:
    """
    Removes erroneus k-mers.
    """
    input:
        fastq = NORM_DIR + "{sample}.keep.{pair}.fq.gz",
        table = NORM_DIR + "diginorm_table.kh"
    output:
        fastq = temp(NORM_DIR + "{sample}.abundfilt.{pair}.fq.gz")
    threads:
        ALL_THREADS
    priority:
        50
    log:
        NORM_DIR + "filter_abund_{sample}_{pair}.log"
    benchmark:
        NORM_DIR + "filter_abunt_{sample}_{pair}.json"
    shell:
        "filter-abund.py "
            "--variable-coverage "
            "--threads {threads} "
            "--output >(pigz --best > {output.fastq}) "
            "{input.table} "
            "<(pigz --decompress --stdout {input.fastq}) "
        "> {log} 2>&1"



rule diginorm_extract_paired_reads_sample:
    """
    Split the filtered reads into PE and SE.
    """
    input:
        fastq = NORM_DIR + "{sample}.abundfilt.pe_pe.fq.gz"
    output:
        fastq_pe = protected(NORM_DIR + "{sample}.final.pe_pe.fq.gz"),
        fastq_se = temp(NORM_DIR + "{sample}.temp.pe_se.fq.gz")
    threads:
        ALL_THREADS
    priority:
        50
    log:
        NORM_DIR + "extract_paired_reads_{sample}.log"
    benchmark:
        NORM_DIR + "extract_paired_reads_{sample}.json"
    shell:
        "extract-paired-reads.py "
            "--output-paired >(pigz --best > {output.fastq_pe}) "
            "--output-single >(pigz --best > {output.fastq_se}) "
            "<(pigz --decompress --stdout {input.fastq}) "
        "> {log} 2>&1 && "
        "sleep 5"


rule diginorm_merge_pe_single_reads_sample:
    """
    Put together the SE reads from the same sample
    """
    input:
        from_norm=   NORM_DIR + "{sample}.abundfilt.pe_se.fq.gz",
        from_paired= NORM_DIR + "{sample}.temp.pe_se.fq.gz"
    output:
        fastq = protected(NORM_DIR + "{sample}.final.pe_se.fq.gz")
    threads:
        ALL_THREADS
    priority:
        50
    log:
        NORM_DIR + "merge_single_reads_{sample}.log"
    benchmark:
        NORM_DIR + "merge_single_reads_{sample}.json"
    shell:
        "pigz --decompress --stdout "
            "{input.from_norm} "
            "{input.from_paired} "
            "| pigz --best "
        "> {output.fastq} 2> {log}"



rule dignorm_get_former_se_reads_sample:
    """
    Move the result of diginorm_extract_paired_reads for true SE reads
    to their final position.
    """
    input:
        single= NORM_DIR + "{sample}.abundfilt.se.fq.gz"
    output:
        single= protected(NORM_DIR + "{sample}.final.se.fq.gz")
    threads:
        1
    priority:
        50
    log:
        NORM_DIR + "get_former_se_reads_{sample}.log"
    benchmark:
        NORM_DIR + "get_former_se_reads_{sample}.json"
    shell:
        "mv {input.single} {output.single}"



rule diginorm_results:
    input:
        expand(
            NORM_DIR + "{sample}.final.{pair}.fq.gz",
            sample = SAMPLES_PE,
            pair = PAIRS
        ),
        expand(
            NORM_DIR + "{sample}.final.se.fq.gz",
            sample = SAMPLES_SE
        )



rule diginorm_fastqc_sample_pair:
    """
    Do FASTQC reports
    Uses --nogroup!
    One thread per fastq.gz file
    """
    input:
        fastq = NORM_DIR + "{sample}.final.{pair}.fq.gz"
    output:
        zip   = protected(NORM_DIR + "{sample}.final.{pair}_fastqc.zip"),
        html  = protected(NORM_DIR + "{sample}.final.{pair}_fastqc.html")
    threads:
        1
    params:
        outdir = NORM_DIR
    log:
        NORM_DIR + "fastqc_{sample}_{pair}.log"
    benchmark:
        NORM_DIR + "fastqc_{sample}_{pair}.json"
    shell:
        "fastqc "
            "--nogroup "
            "--outdir {params.outdir} "
            "{input.fastq} "
        "> {log} 2>&1"



rule diginorm_multiqc:
    input:
        files_pe = expand(
            NORM_DIR + "{sample}.final.{pair}_fastqc.{extension}",
            sample = SAMPLES_PE,
            pair = "pe_pe pe_se".split(),
            extension = "html zip".split()
        ),
        files_se = expand(
            NORM_DIR + "{sample}.final.se_fastqc.{extension}",
            sample = SAMPLES_SE,
            extension = "html zip".split()
        )
    output:
        html= protected(NORM_DIR + "multiqc_report.html")
    params:
        folder = NORM_DIR
    log:
        NORM_DIR + "multiqc.log"
    benchmark:
        NORM_DIR + "multiqc.json"
    shell:
        "multiqc "
            "--title Diginorm "
            "--filename {output.html} "
            "{params.folder} "
        "2> {log}"



rule diginorm_doc:
    input:
        html= NORM_DIR + "multiqc_report.html"



rule diginorm:
    '''diginorm_results + diginorm_doc'''
    input:
        files_pe = expand(
            NORM_DIR + "{sample}.final.{pair}.fq.gz",
            sample = SAMPLES_PE,
            pair = PAIRS
        ),
        files_se = expand(
            NORM_DIR + "{sample}.final.se.fq.gz",
            sample = SAMPLES_SE
        ),
        html= NORM_DIR + "multiqc_report.html"
